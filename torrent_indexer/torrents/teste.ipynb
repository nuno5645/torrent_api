{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from torrent_api import TorrentAPI\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import wget\n",
    "import random\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "\n",
    "# api = TorrentAPI()\n",
    "\n",
    "\n",
    "# api_key = \"2480c2206d4661b89bf222cbc9c7f5ea\"\n",
    "# url = f\"https://api.themoviedb.org/3/movie/popular?api_key={api_key}&language=en-US&page=1\"\n",
    "\n",
    "# df = pq.read_table('TMDB_movie_dataset_v11.parquet')\n",
    "# pandas_df = df.to_pandas()\n",
    "# print(pandas_df)\n",
    "\n",
    "#turn into csv\n",
    "# pandas_df.to_csv('TMDB_movie_dataset_v11.csv', index=False)\n",
    "\n",
    "df = pd.read_csv('TMDB_movie_dataset_v11.csv')\n",
    "\n",
    "print(df.head())\n",
    "# response = requests.get(url)\n",
    "# if response.status_code == 200:\n",
    "#     movie_list = response.json()\n",
    "# else:\n",
    "#     print(f\"Error: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from torrent_api import TorrentAPI\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import wget\n",
    "import random\n",
    "\n",
    "\n",
    "api = TorrentAPI()\n",
    "\n",
    "# Load the parquet file with predicate pushdown\n",
    "filters = [\n",
    "    ('original_language', '=', 'en'),\n",
    "    ('budget', '>', 1000000)\n",
    "]\n",
    "TMDB_dataset = pq.read_table('TMDB_movie_dataset_v11.parquet', filters=filters)\n",
    "\n",
    "# Convert to pandas only after filtering\n",
    "TMDB_dataset = TMDB_dataset.to_pandas()\n",
    "\n",
    "print(TMDB_dataset.head())\n",
    "\n",
    "saved_torrent_names = []\n",
    "\n",
    "\n",
    "\n",
    "# The rest of your code can remain the same, as it uses movie_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# ... existing code ...\n",
    "\n",
    "random_movies = TMDB_dataset.sample(n=7)\n",
    "try:\n",
    "    # Create a movie_list with 20 random movies, including the release year\n",
    "    movie_list = {\n",
    "        'results': [\n",
    "            {\n",
    "                'title': movie['title'],\n",
    "                'release_year': movie['release_date'][:4]  # Extract year from release_date\n",
    "            }\n",
    "            for _, movie in random_movies.iterrows()\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    print(\"Randomly selected movies:\")\n",
    "    for movie in movie_list['results']:\n",
    "        print(f\"{movie['title']} ({movie['release_year']})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Exception occurred: {str(e)}\")\n",
    "    print(\"Error: Movies not found\")\n",
    "    print(random_movies)\n",
    "\n",
    "# ... existing code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "sites = api.get_supported_sites()\n",
    "\n",
    "# sites = [\"piratebay\", \"1337x\", \"kickass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "#mix the order of the list of sites\n",
    "list_of_sites = random.sample(sites['supported_sites'], len(sites['supported_sites']))\n",
    "list_of_sites = [\"piratebay\"]\n",
    "movies = []\n",
    "movie_titles = [f\"{movie['title']} {movie['release_year']}\" for movie in movie_list['results']]\n",
    "title = movie_titles[4]\n",
    "\n",
    "# title = \"The Dark Knight\"\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "def categorize_content(filename):\n",
    "    # cleaned = preprocess_title(filename, extract_year(filename))\n",
    "    cleaned = preprocess_title(filename)\n",
    "    return 'tv-show' if cleaned is None else 'movie'\n",
    "\n",
    "\n",
    "def extract_year(title):\n",
    "    year_match = re.search(r'(\\d{4})', title)\n",
    "    return year_match.group(1) if year_match else ''\n",
    "\n",
    "# def preprocess_title(title, year=None):\n",
    "#     # Check for TV series patterns\n",
    "#     if re.search(r'\\b(s\\d{2}e\\d{2}|season\\s*\\d+|episode\\s*\\d+)\\b', title, re.IGNORECASE):\n",
    "#         return None  # Return None for TV series\n",
    "\n",
    "#     # Convert to lowercase\n",
    "#     title = title.lower()\n",
    "\n",
    "#     # Replace dots with spaces\n",
    "#     title = title.replace('.', ' ')\n",
    "\n",
    "#     # Remove common words and patterns in torrent names\n",
    "#     title = re.sub(\n",
    "#         r'\\b(dvdrip|brrip|bluray|web-dl|webrip|xx265|h264|h265|h.265|5.1|7.1|x265|ddp|BR|PT|hevc|xvid|divx|1080p|720p|480p|2160p|4k|uhd|hdtv|pdtv|hdcam|hdrip|yify|yts|eztv|rarbg|torrent|extended|repack|unrated|proper|remux|amzn|nf|dsnp|hulu|hbo|imax|directors\\.cut|theatrical\\.cut|dc|tc|hc|ts|cam|screener|subbed|dubbed|dual\\.audio|multi|subs|complete|season|episode|s\\d{2}e\\d{2}|s\\d{2}|e\\d{2}|part|trilogy|quadrilogy|limited|internal|retail|untouched|aac|ac3|dts|truehd|atmos|flac|mp3|dd5\\.1|6ch|10bit|8bit|hdr|sdr|dolby\\.vision|dv|60fps|3d|half-sbs|full-sbs|extended\\.cut|final\\.cut|uncut|theatrical|alternate\\.ending|deleted\\.scenes|commentary|extras|featurettes|documentary|sample|r5|vhsrip|workprint|preair|ddp5\\.1|dvdscr|tv|dd2\\.0|regraded|repack|upscaled|bdrip|brrip|multi-language|webcap|stv|prores|mastered|criterion|french|german|italian|spanish|japanese|korean|english|portuguese|russian|chinese|thai|dutch|norwegian|swedish|finnish|danish|pl|cz|sk|uk|remastered|rm4k|eac3|vff|eng|10bits|pirates|satori|ghost|tigole|t0m)\\b',\n",
    "#         '',\n",
    "#         title,\n",
    "#         flags=re.IGNORECASE\n",
    "#     )\n",
    "#     # Remove special characters\n",
    "#     title = re.sub(r'[^\\w\\s]', '', title)\n",
    "#     # Remove extra spaces\n",
    "\n",
    "#     if year:\n",
    "#         # Remove year if present (assuming format: (YYYY) or [YYYY] or in the middle of the title)\n",
    "#         title = re.sub(r'(\\d{4})', '', title)\n",
    "#         title = f\"{title} {year}\"\n",
    "\n",
    "#     title = ' '.join(title.split())\n",
    "\n",
    "#     return title\n",
    "\n",
    "# def preprocess_title(filename):\n",
    "#     # Remove file extension\n",
    "#     name = re.sub(r'\\.[^.]+$', '', filename)\n",
    "\n",
    "#     # Remove year\n",
    "#     name = re.sub(r'\\s*[\\[\\(]?(?:19|20)\\d{2}[\\]\\)]?', '', name)\n",
    "\n",
    "#     name = re.sub(r'(\\.|\\s)(19\\d{2}|20\\d{2}|[0-9]{3,4}p|4K|UHD|HD|SD|HDTV|DVD|Blu-?Ray|WEB|WEB-?DL|WEBRip|REMUX|REPACK|PROPER|EXTENDED|UNCUT|THEATRICAL|DC|Directors\\.Cut|Final\\.Cut|Remastered|UNRATED|HEVC|[xh]\\.?26[45]|10bit|8bit|HDR|HDR10\\+?|DTS(-HD)?|DD\\+?|MA|TrueHD|Atmos|DDP?[0-9]\\.[0-9]|AAC[0-9]\\.[0-9]|AC-?3|FLAC|IMAX|HYBRID|MULTI|DUAL|ViSiON|AMZN|NF|DSNP|HMAX|ATVP|COMPLETE|iNTERNAL|LiMiTED|DV|DoVi|AVC|VC-?1|Open\\.Matte|Upscale(d)?|AI(-Enhanced)?|[0-9]+MB|[0-9]+GB|[0-9]+fps|S[0-9]{2}|E[0-9]{2}|EP[0-9]{2}|Season\\.?[0-9]+|Episode\\.?[0-9]+|S[0-9]{2}E[0-9]{2}|Part\\.?[0-9]+|DISC[0-9]+|D[0-9]+|CD[0-9]+|R[0-9]|NTSC|PAL|PDTV|STV|SDTV|XXX|TOPKEK|KILLERS|FGT|YIFY|YTS|RARBG|ExKinoRay|BluRay\\.com|MkvCage|MeGusta|CiNEFiLE|GECKOS|ROVERS|VYNDROS|HDMaNiAcS|CtrlHD|LoRD|DEPTH|JOY|-[a-zA-Z0-9]+|-NOGROUP|\\.[a-zA-Z0-9]+|v[0-9]|Ver\\.[0-9]|\\[[^\\]]+\\]|\\([^\\)]+\\)).*$', '', name)\n",
    "\n",
    "#     # Extensive list of quality indicators, codecs, and technical info\n",
    "#     tech_info = r'\\b(2160p|1080p|720p|480p|360p|4K|UHD|HD|SD|HDR10?\\+?|HDR|DV|DoVi|Dolby Vision|BluRay|Blu-Ray|WEB-DL|WEBRip|DVDRip|BrRip|MB|BDRip|REMUX|REPACK|PROPER|iNTERNAL|AMZN|DSNP|NF|ATVP|HULU|HMAX|HEVC|AVC|DDP?\\d?\\.?\\d?|DTS-?HD|DTS-?X|DTS|DD|Atmos|TrueHD|AC3|AAC|EAC3|FLAC|MP3|OPUS|x264|x265|H\\.?264|H\\.?265|10bit|8bit|Hi10P|Hi10|480i|480p|540p|576p|720p|1080i|1080p|2160p|BT2020|P3|BT709|HDR|SDR|PQ|HLG)'\n",
    "#     name = re.sub(tech_info, '', name, flags=re.IGNORECASE)\n",
    "\n",
    "#     # Remove release group tags and other common suffixes\n",
    "#     groups = r'\\b(YIFY|YTS|RARBG|FGT|MeGusta|CMRG|FLUX|EDITH|ETHEL|cakes|TEPES|Silence|Joy|Tigole|BONE|NAHOM|BiTOR|NTb|BTN|NOGRP|CiNEPHiLES|IAMABLE|FraMeSToR|CtrlHD|ESiR|TERMiNAL|DEPTH|MkvCage|PSA|SWTYBLZ|HUZZAH|VYNDROS|EMBER|VYNDROS|TORRENTGALAXY|TGx|EVO|SMURF|MZABI|RTFM|DEFLATE|WDYM)'\n",
    "#     name = re.sub(groups, '', name, flags=re.IGNORECASE)\n",
    "\n",
    "#     # Remove special characters and extra spaces\n",
    "#     name = re.sub(r'[.\\-_\\[\\](){}]', ' ', name)\n",
    "\n",
    "#     # Remove file size\n",
    "#     # Remove file size\n",
    "#     name = re.sub(r'\\b\\d+(?:\\.\\d+)?\\s*(?:GB|MB|KB)\\b', '', name, flags=re.IGNORECASE)\n",
    "\n",
    "#     # Remove season and episode information\n",
    "#     name = re.sub(r'\\b(S\\d{1,3}E\\d{1,3}|S\\d{1,3}|E\\d{1,3}|EP\\d{1,3}|SEASON \\d{1,3}|EPISODE \\d{1,3}|PART \\d+|Ch\\d+)\\b', '', name, flags=re.IGNORECASE)\n",
    "\n",
    "#     # Remove common words that aren't part of the title\n",
    "#     common_words = r'\\b(EXTENDED|UNRATED|DC|THEATRICAL|CUT|EDITION|VERSION|REMASTERED|DIRECTORS|HYBRID|COMPLETE|REMUX|LIMITED|RESTORED|CRITERION|COLLECTION|DISC \\d+|EXTRAS|BONUS|SPECIAL|FEATURES|DOCUMENTARY|INTERVIEW|MAKING OF|ANNIVERSARY|DELUXE|SUPERBIT|RETAIL|IMAX|FINAL CUT|UNCUT)'\n",
    "#     name = re.sub(common_words, '', name, flags=re.IGNORECASE)\n",
    "\n",
    "#     # Remove language indicators\n",
    "#     languages = r'\\b(MULTi|DUAL|iTA|ENG|ESP|RUS|FRE|GER|HINDI|LATINO|TAMIL|TELUGU|ITA|JAP|POLISH|PORTUGUESE|TRUEFRENCH|SUBBED|DUBBED|SUB|DUB)'\n",
    "#     name = re.sub(languages, '', name, flags=re.IGNORECASE)\n",
    "\n",
    "#     # Remove additional patterns\n",
    "#     name = re.sub(r'\\b(READY PLAYER ONE|BEN THE MEN|AI Upscaled|AI Enhanced|Dolby Vision|IMAX|SDR|HYBRID|AI Enhance|Ai|Custom|Special Edition|Remastered|Restored|Criterion|Ultimate|COMPLETE|Premium|REMIX)\\b', '', name, flags=re.IGNORECASE)\n",
    "\n",
    "#     # Remove any remaining parentheses, square brackets, and curly braces and their contents\n",
    "#     name = re.sub(r'[\\(\\[\\{].*?[\\)\\]\\}]', '', name)\n",
    "\n",
    "#     # Remove common file prefixes\n",
    "#     name = re.sub(r'^(www\\.[a-zA-Z0-9]+\\.(com|org|net)[-_])', '', name, flags=re.IGNORECASE)\n",
    "\n",
    "#     # Remove trailing year if it still exists\n",
    "#     name = re.sub(r'\\s*[\\[\\(]?(?:19|20)\\d{2}[\\]\\)]?$', '', name)\n",
    "\n",
    "#     # Remove common torrent site tags\n",
    "#     name = re.sub(r'\\[?(?:ettv|eztv|rarbg|yify|yts)\\]?', '', name, flags=re.IGNORECASE)\n",
    "\n",
    "#     # Remove resolution if it's at the start or end\n",
    "#     name = re.sub(r'^(?:2160p|1080p|720p|480p|360p)\\s+', '', name, flags=re.IGNORECASE)\n",
    "#     name = re.sub(r'\\s+(?:2160p|1080p|720p|480p|360p)$', '', name, flags=re.IGNORECASE)\n",
    "\n",
    "#     # Remove extra spaces\n",
    "#     name = re.sub(r'\\s+', ' ', name).strip()\n",
    "#     name = ' '.join(name.split())\n",
    "\n",
    "#     return name\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_title(filename):\n",
    "    \n",
    "    if re.search(r'\\b(S\\d{1,3}E\\d{1,3}|S\\d{1,3}|E\\d{1,3}|EP\\d{1,3}|SEASON \\d{1,3}|EPISODE \\d{1,3}|m2ts|COMPLETE SERIES)\\b', filename, flags=re.IGNORECASE):\n",
    "        return None\n",
    "    \n",
    "    name = re.sub(r'[^\\x00-\\x7F]+', '', filename)\n",
    "\n",
    "    # Remove file extension\n",
    "    name = re.sub(r'\\.[^.]+$', '', filename)\n",
    "\n",
    "    # Remove year\n",
    "    name = re.sub(r'\\s*[\\[\\(]?(?:19|20)\\d{2}[\\]\\)]?', '', name)\n",
    "\n",
    "    name = re.sub(r'(\\.|\\s)(19\\d{2}|20\\d{2}|[0-9]{3,4}p|4K|UHD|HD|SD|HDTV|DVD|Blu-?Ray|WEB|WEB-?DL|WEBRip|REMUX|REPACK|PROPER|EXTENDED|UNCUT|THEATRICAL|DC|Directors\\.Cut|Final\\.Cut|Remastered|UNRATED|HEVC|[xh]\\.?26[45]|10bit|8bit|HDR|HDR10\\+?|DTS(-HD)?|DD\\+?|MA|TrueHD|Atmos|DDP?[0-9]\\.[0-9]|AAC[0-9]\\.[0-9]|AC-?3|FLAC|IMAX|HYBRID|MULTI|DUAL|ViSiON|AMZN|NF|DSNP|HMAX|ATVP|COMPLETE|iNTERNAL|LiMiTED|DV|DoVi|AVC|VC-?1|Open\\.Matte|Upscale(d)?|AI(-Enhanced)?|[0-9]+MB|[0-9]+GB|[0-9]+fps|S[0-9]{2}|E[0-9]{2}|EP[0-9]{2}|Season\\.?[0-9]+|Episode\\.?[0-9]+|S[0-9]{2}E[0-9]{2}|Part\\.?[0-9]+|DISC[0-9]+|D[0-9]+|CD[0-9]+|R[0-9]|NTSC|PAL|PDTV|STV|SDTV|XXX|TOPKEK|KILLERS|FGT|YIFY|YTS|RARBG|ExKinoRay|BluRay\\.com|MkvCage|MeGusta|CiNEFiLE|GECKOS|ROVERS|VYNDROS|HDMaNiAcS|CtrlHD|LoRD|DEPTH|JOY|-[a-zA-Z0-9]+|-NOGROUP|\\.[a-zA-Z0-9]+|v[0-9]|Ver\\.[0-9]|\\[[^\\]]+\\]|\\([^\\)]+\\)).*$', '', name)\n",
    "\n",
    "    # Extensive list of quality indicators, codecs, and technical info\n",
    "    tech_info = r'\\b(2160p|1080p|720p|480p|360p|4K|UHD|HD|SD|HDR10?\\+?|HDR|DV|DoVi|Dolby Vision|BluRay|Blu-Ray|WEB-DL|WEBRip|DVDRip|BrRip|MB|BDRip|REMUX|REPACK|PROPER|iNTERNAL|AMZN|DSNP|NF|ATVP|HULU|HMAX|HEVC|AVC|DDP?\\d?\\.?\\d?|DTS-?HD|DTS-?X|DTS|DD|Atmos|TrueHD|AC3|AAC|EAC3|FLAC|MP3|OPUS|x264|x265|H\\.?264|H\\.?265|10bit|8bit|Hi10P|Hi10|480i|480p|540p|576p|720p|1080i|1080p|2160p|BT2020|P3|BT709|HDR|SDR|PQ|HLG)'\n",
    "    name = re.sub(tech_info, '', name, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove release group tags and other common suffixes\n",
    "    groups = r'\\b(YIFY|YTS|RARBG|FGT|MeGusta|CMRG|FLUX|EDITH|ETHEL|cakes|TEPES|Silence|Joy|Tigole|BONE|NAHOM|BiTOR|NTb|BTN|NOGRP|CiNEPHiLES|IAMABLE|FraMeSToR|CtrlHD|ESiR|TERMiNAL|DEPTH|MkvCage|PSA|SWTYBLZ|HUZZAH|VYNDROS|EMBER|VYNDROS|TORRENTGALAXY|TGx|EVO|SMURF|MZABI|RTFM|DEFLATE|WDYM)'\n",
    "    name = re.sub(groups, '', name, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove special characters and extra spaces\n",
    "    name = re.sub(r'[.\\-_\\[\\](){}]', ' ', name)\n",
    "\n",
    "    # Remove file size\n",
    "    name = re.sub(r'\\b\\d+(?:\\.\\d+)?\\s*(?:GB|MB|KB)\\b', '', name, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove season and episode information\n",
    "    name = re.sub(r'\\b(S\\d{1,3}E\\d{1,3}|S\\d{1,3}|E\\d{1,3}|EP\\d{1,3}|SEASON \\d{1,3}|EPISODE \\d{1,3}|PART \\d+|Ch\\d+)\\b', '', name, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove common words that aren't part of the title\n",
    "    common_words = r'\\b(EXTENDED|UNRATED|DC|THEATRICAL|CUT|EDITION|VERSION|REMASTERED|DIRECTORS|HYBRID|COMPLETE|REMUX|LIMITED|RESTORED|CRITERION|COLLECTION|DISC \\d+|EXTRAS|BONUS|SPECIAL|FEATURES|DOCUMENTARY|INTERVIEW|MAKING OF|ANNIVERSARY|DELUXE|SUPERBIT|RETAIL|IMAX|FINAL CUT|UNCUT)'\n",
    "    name = re.sub(common_words, '', name, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove language indicators\n",
    "    languages = r'\\b(MULTi|DUAL|iTA|ENG|ESP|RUS|FRE|GER|HINDI|LATINO|TAMIL|TELUGU|ITA|JAP|POLISH|PORTUGUESE|TRUEFRENCH|SUBBED|DUBBED|SUB|DUB)'\n",
    "    name = re.sub(languages, '', name, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove additional patterns\n",
    "    name = re.sub(r'\\b(READY PLAYER ONE|BEN THE MEN|AI Upscaled|AI Enhanced|Dolby Vision|IMAX|SDR|HYBRID|AI Enhance|Ai|Custom|Special Edition|Remastered|Restored|Criterion|Ultimate|COMPLETE|Premium|REMIX)\\b', '', name, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove any remaining parentheses, square brackets, and curly braces and their contents\n",
    "    name = re.sub(r'[\\(\\[\\{].*?[\\)\\]\\}]', '', name)\n",
    "\n",
    "    # Remove common file prefixes\n",
    "    name = re.sub(r'^(www\\.[a-zA-Z0-9]+\\.(com|org|net)[-_])', '', name, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove trailing year if it still exists\n",
    "    name = re.sub(r'\\s*[\\[\\(]?(?:19|20)\\d{2}[\\]\\)]?$', '', name)\n",
    "\n",
    "    # Remove common torrent site tags\n",
    "    name = re.sub(r'\\[?(?:ettv|eztv|rarbg|yify|yts)\\]?', '', name, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove resolution if it's at the start or end\n",
    "    name = re.sub(r'^(?:2160p|1080p|720p|480p|360p)\\s+', '', name, flags=re.IGNORECASE)\n",
    "    name = re.sub(r'\\s+(?:2160p|1080p|720p|480p|360p)$', '', name, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove Chinese and other non-Latin characters\n",
    "    name = re.sub(r'[\\u4e00-\\u9fff\\u3000-\\u303f\\u3040-\\u309f\\u30a0-\\u30ff\\uff00-\\uff9f]', '', name)\n",
    "\n",
    "    # Remove extra spaces\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    name = ' '.join(name.split())\n",
    "\n",
    "    return name\n",
    "\n",
    "\n",
    "def calculate_similarity(title1, title2):\n",
    "    processed_title1 = preprocess_title(title1)\n",
    "    # processed_title2 = preprocess_title(title2, extract_year(title2))\n",
    "    processed_title2 = preprocess_title(title2)\n",
    "    print(\"Original title: \", processed_title1)\n",
    "    print(\"Torrent title: \", processed_title2)\n",
    "\n",
    "    tfidf = vectorizer.fit_transform([processed_title1, processed_title2])\n",
    "    return cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0]\n",
    "\n",
    "similarity_threshold = 0.5  # Adjust this value as needed\n",
    "\n",
    "for i in list_of_sites:\n",
    "    try:\n",
    "        title = \"Inception 2010\"\n",
    "        results = api.search(f\"{i}\", f\"{title}\", limit=5)  # Now includes year in the search\n",
    "        if not results or 'error' in results or not results['data']:\n",
    "            print(f\"No results for {i}\\n\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Results for {i}:\")\n",
    "        for result in results['data']:\n",
    "            print(result)\n",
    "            found_title = result['name']\n",
    "\n",
    "            if found_title not in saved_torrent_names:\n",
    "                saved_torrent_names.append(found_title)\n",
    "\n",
    "\n",
    "            similarity = calculate_similarity(title, found_title)\n",
    "\n",
    "            print(f\"Similarity: {similarity}\")\n",
    "\n",
    "            if similarity >= similarity_threshold:\n",
    "                movie_dict = {\n",
    "                    \"site\": i,\n",
    "                    \"name\": found_title,\n",
    "                    \"size\": result['size'],\n",
    "                    \"url\": result['url'],\n",
    "                    \"hash\": result['hash'],\n",
    "                    \"magnet\": result['magnet'],\n",
    "                    \"similarity\": similarity\n",
    "                }\n",
    "\n",
    "                movies.append(movie_dict)\n",
    "                print(json.dumps(result, indent=2))\n",
    "                print(\"\\n\")\n",
    "            else:\n",
    "                ...\n",
    "                # print(f\"Similarity too low, skipping...\\n\")\n",
    "\n",
    "        if movies:\n",
    "            break  # Break after processing all results from a site with at least one match\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # print(f\"Error for {i}\")\n",
    "        # print(results)\n",
    "\n",
    "print(\"Final list of movies:\")\n",
    "for movie in movies:\n",
    "    print(f\"- {movie['name']} (Similarity: {movie['similarity']:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import wget\n",
    "from RD import RealDebridAPI\n",
    "\n",
    "real_debrid = RealDebridAPI(\"B4YMPW225WYGYYOOSRNEHFQX33WWVQNY7IWCO54XTVSQYNYJEY3Q\")\n",
    "\n",
    "# Check instant availability for the first movie\n",
    "if movies:\n",
    "    first_movie_hash = movies[0]['hash']\n",
    "    availability = real_debrid.get_torrent_instant_availability(first_movie_hash)\n",
    "    print(f\"\\nInstant availability for {movies[0]['name']}:\")\n",
    "    print(json.dumps(availability, indent=2))\n",
    "else:\n",
    "    print(\"\\nNo movies found to check instant availability.\")\n",
    "\n",
    "# Adding torrent\n",
    "magnet_link = movies[0]['magnet']\n",
    "print(\"Adding torrent...\")\n",
    "torrent_response = real_debrid.add_magnet(magnet_link)\n",
    "print(f\"Response from adding torrent: {torrent_response}\")  # Added print\n",
    "torrent_id = torrent_response['id']\n",
    "\n",
    "# Getting torrent info\n",
    "print(\"Getting torrent info...\")\n",
    "file_info = real_debrid.get_torrent_info(torrent_id)\n",
    "file_status = file_info['status']\n",
    "print(f\"Initial file status: {file_status}\")  # Added print\n",
    "\n",
    "# Waiting for Magnet conversion\n",
    "print(\"Waiting for Magnet conversion...\")\n",
    "while file_status == \"magnet_conversion\":\n",
    "    time.sleep(15)\n",
    "    file_info = real_debrid.get_torrent_info(torrent_id)\n",
    "    file_status = file_info['status']\n",
    "    print(f\"Current file status: {file_status}\")  # Added print\n",
    "\n",
    "# Selecting Files\n",
    "print(\"Conversion done. Selecting Files...\")\n",
    "files = file_info['files']\n",
    "print(f\"Total files available: {len(files)}\")  # Added print\n",
    "biggest_file = max(files, key=lambda x: x['bytes'])\n",
    "biggest_file_index = files.index(biggest_file)\n",
    "biggest_file_name = biggest_file['path']\n",
    "\n",
    "print(f\"Torrenting now: {biggest_file_name}\")\n",
    "print(f\"Torrent ID: {torrent_id}\")  # Added print\n",
    "print(f\"Biggest file index: {biggest_file_index + 1}\")  # Added print\n",
    "\n",
    "select_result = real_debrid.select_files(torrent_id, biggest_file_index + 1)\n",
    "print(f\"Select result: {select_result}\")  # Added print\n",
    "\n",
    "# Waiting for torrent to finish\n",
    "print(\"Waiting for torrent to finish...\")\n",
    "file_info = real_debrid.get_torrent_info(torrent_id)\n",
    "print(f\"File info after waiting: {file_info}\")  # Added print\n",
    "file_status = file_info['status']\n",
    "\n",
    "\n",
    "\n",
    "while file_status == \"queued\" or file_status == \"downloading\":\n",
    "    time.sleep(15)\n",
    "    file_info = real_debrid.get_torrent_info(torrent_id)\n",
    "    file_status = file_info['status']\n",
    "    if file_info.get('links'):\n",
    "        print(file_info['links'])\n",
    "    print(f\"Current file status: {file_status}\")  # Added print\n",
    "\n",
    "\n",
    "# Downloading\n",
    "print(\"Torrent is finished. Downloading now...\")\n",
    "print(file_info)\n",
    "\n",
    "\n",
    "\n",
    "rd_link = file_info['links'][0]\n",
    "download_link = real_debrid.unrestrict_link(rd_link)\n",
    "\n",
    "print(download_link)\n",
    "\n",
    "# print(download_link)\n",
    "transcode = real_debrid.get_streaming_transcode(file_info['id'])\n",
    "\n",
    "print(transcode)\n",
    "# print(\"Starting download...\")\n",
    "# # wget.download(download_link['download'])\n",
    "# print(\"\\nDownload done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results_list = []\n",
    "\n",
    "# Loop through all movies in movie_titles\n",
    "for title in movie_titles:\n",
    "    print(f\"\\nProcessing: {title}\")\n",
    "    \n",
    "    for i in list_of_sites:\n",
    "        try:\n",
    "            results = api.search(f\"{i}\", f\"{title}\", limit=5)\n",
    "            if not results or 'error' in results or not results['data']:\n",
    "                print(f\"No results for {i}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Results for {i}:\")\n",
    "            for result in results['data']:\n",
    "                found_title = result['name']\n",
    "                cleaned_title = preprocess_title(found_title)\n",
    "                similarity = calculate_similarity(title, found_title)\n",
    "                if found_title not in saved_torrent_names:\n",
    "                    saved_torrent_names.append(found_title)\n",
    "\n",
    "                print(f\"Original: {title}\")\n",
    "                print(f\"Found: {found_title}\")\n",
    "                print(f\"Cleaned: {cleaned_title}\")\n",
    "                print(f\"Similarity: {similarity}\")\n",
    "                print(\"---\")\n",
    "\n",
    "                # Append result to the list\n",
    "                results_list.append({\n",
    "                    'original_title': title,\n",
    "                    'filename': found_title,\n",
    "                    'cleaned': cleaned_title,\n",
    "                    'similarity': similarity\n",
    "                })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error for {i}: {str(e)}\")\n",
    "\n",
    "    print(f\"Finished processing: {title}\")\n",
    "\n",
    "# Create DataFrame from results\n",
    "df_results = pd.DataFrame(results_list)\n",
    "\n",
    "# Sort by similarity in descending order\n",
    "df_results = df_results.sort_values('similarity', ascending=False)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df_results.head())\n",
    "\n",
    "# Save to CSV (optional)\n",
    "df_results.to_csv('movie_search_results.csv', index=False)\n",
    "print(\"Results saved to 'movie_search_results.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "print(len(saved_torrent_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get all the files on my download folder in real debrid\n",
    "all_downloads = []\n",
    "page = 1\n",
    "limit = 100  # You can adjust this value based on your needs\n",
    "\n",
    "while True:\n",
    "    downloads = real_debrid.get_downloads(page=page, limit=limit)\n",
    "    if not downloads:\n",
    "        break\n",
    "    all_downloads.extend(downloads)\n",
    "    page += 1\n",
    "\n",
    "# Create a DataFrame with relevant information\n",
    "df_downloads = pd.DataFrame(all_downloads)\n",
    "\n",
    "# Select relevant columns\n",
    "columns_to_keep = ['filename', 'filesize', 'generated', 'streamable']\n",
    "df_downloads = df_downloads[columns_to_keep]\n",
    "\n",
    "# df_downloads['filename_cleaned'] = df_downloads['filename'].apply(lambda x: preprocess_title(x, extract_year(x)))\n",
    "df_downloads['filename_cleaned'] = df_downloads['filename'].apply(lambda x: preprocess_title(x))\n",
    "df_downloads['category'] = df_downloads['filename'].apply(categorize_content)\n",
    "\n",
    "# Remove '.mkv' extension from filenames\n",
    "df_downloads['filename_cleaned'] = df_downloads['filename_cleaned'].str.replace('.mkv', '', regex=False).str.replace('mkv', '', regex=False)\n",
    "\n",
    "# Convert generated to datetime\n",
    "df_downloads['generated'] = pd.to_datetime(df_downloads['generated'])\n",
    "\n",
    "# Sort by generated date (most recent first)\n",
    "df_downloads = df_downloads.sort_values('generated', ascending=True)\n",
    "\n",
    "# Reset index\n",
    "df_downloads = df_downloads.reset_index(drop=True)\n",
    "\n",
    "# Display the first few rows and basic information about the DataFrame\n",
    "print(df_downloads.head())\n",
    "\n",
    "# Write filenames to a text file\n",
    "with open('real_debrid_filenames.txt', 'w', encoding='utf-8') as f:\n",
    "    for filename in df_downloads['filename']:\n",
    "        f.write(f\"{filename}\\n\")\n",
    "\n",
    "print(\"Filenames have been written to 'real_debrid_filenames.txt'\")\n",
    "\n",
    "\n",
    "\n",
    "# Optional: Save to CSV\n",
    "df_downloads.to_csv('real_debrid_downloads.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_downloads_no_tvshows = df_downloads[df_downloads['category'] != 'tv-show']\n",
    "df_downloads_no_tvshows.to_csv('real_debrid_downloads_no_tvshows.csv', index=False)\n",
    "print(\"Downloads without TV shows have been saved to 'real_debrid_downloads_no_tvshows.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching Downloads with TMDB Dataset\n",
    "This section of the code is responsible for matching the downloaded files with their corresponding titles in the TMDB dataset. It uses the fuzzywuzzy library to find the best match based on the token sort ratio. The matches are then saved to a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to find the best match\n",
    "def find_best_match(title, dataset):\n",
    "    best_match = None\n",
    "    best_ratio = 0\n",
    "    for _, movie in dataset.iterrows():\n",
    "        ratio = fuzz.token_sort_ratio(title.lower(), movie['title'].lower())\n",
    "        if ratio > best_ratio:\n",
    "            best_ratio = ratio\n",
    "            best_match = movie\n",
    "    return best_match, best_ratio\n",
    "\n",
    "# Find matches for each download\n",
    "matches = []\n",
    "total_downloads = len(df_downloads)\n",
    "\n",
    "# Create a progress bar\n",
    "with tqdm(total=total_downloads, desc=\"Matching downloads\") as pbar:\n",
    "    for _, download in df_downloads.iterrows():\n",
    "        best_match, ratio = find_best_match(download['filename_cleaned'], TMDB_dataset)\n",
    "        if best_match is not None:\n",
    "            matches.append({\n",
    "                'download_filename': download['filename'],\n",
    "                'cleaned_filename': download['filename_cleaned'],\n",
    "                'matched_title': best_match['title'],\n",
    "                'match_ratio': ratio,\n",
    "                'release_date': best_match['release_date'],\n",
    "                'vote_average': best_match['vote_average']\n",
    "            })\n",
    "        pbar.update(1)\n",
    "\n",
    "# Create a DataFrame from the matches\n",
    "df_matches = pd.DataFrame(matches)\n",
    "\n",
    "# Sort by match ratio in descending order\n",
    "df_matches = df_matches.sort_values('match_ratio', ascending=False)\n",
    "\n",
    "# Display the top matches\n",
    "print(df_matches.head(20))\n",
    "\n",
    "# Save the matches to a CSV file\n",
    "df_matches.to_csv('real_debrid_tmdb_matches.csv', index=False)\n",
    "print(\"Matches have been saved to 'real_debrid_tmdb_matches.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#print the total number and percentage of ratio\n",
    "# Calculate statistics for different match ratio thresholds\n",
    "thresholds = [90, 80, 70, 60, 50]\n",
    "total_matches = len(df_matches)\n",
    "\n",
    "print(f\"Total number of matches: {total_matches}\")\n",
    "\n",
    "for threshold in thresholds:\n",
    "    matches_above_threshold = df_matches[df_matches['match_ratio'] >= threshold]\n",
    "    num_matches = len(matches_above_threshold)\n",
    "    percentage = (num_matches / total_matches) * 100\n",
    "    \n",
    "    print(f\"Matches with ratio >= {threshold}: {num_matches} ({percentage:.2f}%)\")\n",
    "\n",
    "# Calculate the average match ratio\n",
    "average_ratio = df_matches['match_ratio'].mean()\n",
    "print(f\"\\nAverage match ratio: {average_ratio:.2f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torrent Name Cleaning with Google Generative AI\n",
    "\n",
    "This section of the code utilizes the Google Generative AI model to clean torrent file names by extracting the movie title and year of release. The process involves removing unnecessary information such as quality indicators, file formats, source tags, and any special characters. The cleaned names are then stored in a new column in the DataFrame.\n",
    "\n",
    "## Key Components:\n",
    "- **API Configuration**: Set up the Google API key for accessing the Generative AI model.\n",
    "- **Model Initialization**: Initialize the Gemini model for generating cleaned movie names.\n",
    "- **Function Definition**: The `clean_torrent_name` function processes each torrent name and returns a cleaned version.\n",
    "- **DataFrame Update**: Apply the cleaning function to the DataFrame containing torrent filenames, creating a new column for the cleaned names.\n",
    "\n",
    "## Example Usage:\n",
    "The code demonstrates how to apply the cleaning function to a sample of torrent names from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import json\n",
    "import typing_extensions as typing\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Configure your API key\n",
    "GOOGLE_API_KEY = \"AIzaSyDP_SI0G2WWAk8EnjlGR5juZDwnYt9XjFU\"\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# Define the schema for the cleaned movie name\n",
    "class CleanedMovieName(typing.TypedDict):\n",
    "    movie_name: str\n",
    "\n",
    "# Initialize the Gemini model\n",
    "model = genai.GenerativeModel(model_name=\"models/gemini-1.5-flash\")\n",
    "\n",
    "def clean_torrent_name(torrent_name):\n",
    "    prompt = f\"\"\"\n",
    "    From the following torrent file name:\n",
    "    \"{torrent_name}\"\n",
    "\n",
    "    Extract and return the clean movie title along with the year of release. Remove any additional information such as:\n",
    "    - Quality indicators (e.g., 720p, 1080p, HDRip)\n",
    "    - File format (e.g., .mp4, .mkv)\n",
    "    - Source tags (e.g., BluRay, WEBRip)\n",
    "    - Group or uploader names\n",
    "    - Any other non-title information\n",
    "    - The year of release\n",
    "    - Any caracteres like [, ], (), /, -, _, +, !, @, #, $, %, ^, &, *, (, ), =, +, , [, ], |, \\, :, \";, \", ', `, ~,  .\n",
    "\n",
    "    Ensure the output includes only the movie title in a clean format.\n",
    "\"\"\"\n",
    "\n",
    "    result = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config=genai.GenerationConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=CleanedMovieName\n",
    "        ),\n",
    "        safety_settings=[\n",
    "            {\"category\": \"HARM_CATEGORY_SEXUAL\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_DANGEROUS\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"}\n",
    "        ],\n",
    "        request_options={\"timeout\": 600},\n",
    "    )\n",
    "\n",
    "    cleaned_name = json.loads(result.text)\n",
    "\n",
    "    movie_name = f\"{cleaned_name['movie_name']}\"\n",
    "    print(f\"Cleaned movie name: {movie_name}\")\n",
    "    return movie_name\n",
    "\n",
    "# Create a new column for cleaned movie names with the year\n",
    "df_downloads_no_tvshows['cleaned_movie_name_with_year'] = df_downloads_no_tvshows['filename'].progress_apply(clean_torrent_name)\n",
    "\n",
    "# # Print the cleaned movie names with year\n",
    "# for i in torrent_names:\n",
    "#     cleaned_movie_name_with_year = clean_torrent_name(i)\n",
    "#     print(f\"Original torrent name: {i}\")\n",
    "#     print(f\"Cleaned movie name with year: {cleaned_movie_name_with_year}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import json\n",
    "import typing_extensions as typing\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# ... (keep your API key configuration and model initialization)\n",
    "\n",
    "class CleanedMovieNames(typing.TypedDict):\n",
    "    movie_names: typing.List[str]\n",
    "\n",
    "def clean_torrent_names(torrent_names):\n",
    "    prompt = f\"\"\"\n",
    "    Below are 10 torrent file names. For each one, extract and return only the clean movie title.\n",
    "    Remove any additional information such as:\n",
    "    - Quality indicators (e.g., 720p, 1080p, HDRip)\n",
    "    - File format (e.g., .mp4, .mkv)\n",
    "    - Source tags (e.g., BluRay, WEBRip)\n",
    "    - Group or uploader names\n",
    "    - Any other non-title information\n",
    "    - The year of release\n",
    "    - Any characters like [, ], (), /, -, _, +, !, @, #, $, %, ^, &, *, (, ), =, +, , [, ], |, \\, :, \";, \", ', `, ~, .\n",
    "\n",
    "    Input torrent names:\n",
    "    {json.dumps(torrent_names)}\n",
    "\n",
    "    Return exactly 10 cleaned movie titles in the same order as the input. If you can't determine a title, use 'Unknown'.\n",
    "    Ensure your response contains exactly 10 movie titles, no more, no less.\n",
    "    \"\"\"\n",
    "\n",
    "    result = model.generate_content(\n",
    "        prompt,\n",
    "        generation_config=genai.GenerationConfig(\n",
    "            response_mime_type=\"application/json\",\n",
    "            response_schema=CleanedMovieNames\n",
    "        ),\n",
    "        safety_settings=[\n",
    "            {\"category\": \"HARM_CATEGORY_SEXUAL\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_DANGEROUS\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
    "            {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"}\n",
    "        ],\n",
    "        request_options={\"timeout\": 600},\n",
    "    )\n",
    "\n",
    "    cleaned_names = json.loads(result.text)\n",
    "    return cleaned_names['movie_names']\n",
    "\n",
    "def process_in_batches(df, batch_size=10):\n",
    "    cleaned_names = []\n",
    "    for i in tqdm(range(0, len(df), batch_size)):\n",
    "        batch = df['filename'].iloc[i:i+batch_size].tolist()\n",
    "        try:\n",
    "            cleaned_batch = clean_torrent_names(batch)\n",
    "            if len(cleaned_batch) != len(batch):\n",
    "                print(f\"Warning: Batch starting at index {i} returned {len(cleaned_batch)} results instead of {len(batch)}\")\n",
    "                # Pad with 'Unknown' if necessary\n",
    "                cleaned_batch += ['Unknown'] * (len(batch) - len(cleaned_batch))\n",
    "            cleaned_names.extend(cleaned_batch)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting at index {i}: {str(e)}\")\n",
    "            # If an error occurs, fill with 'Unknown'\n",
    "            cleaned_names.extend(['Unknown'] * len(batch))\n",
    "    return cleaned_names\n",
    "\n",
    "# Process the dataframe\n",
    "cleaned_movie_names = process_in_batches(df_downloads_no_tvshows)\n",
    "\n",
    "# Check if the lengths match\n",
    "if len(cleaned_movie_names) != len(df_downloads_no_tvshows):\n",
    "    print(f\"Warning: Number of cleaned names ({len(cleaned_movie_names)}) does not match number of rows in dataframe ({len(df_downloads_no_tvshows)})\")\n",
    "    # Pad or truncate the list to match the dataframe length\n",
    "    if len(cleaned_movie_names) < len(df_downloads_no_tvshows):\n",
    "        cleaned_movie_names += ['Unknown'] * (len(df_downloads_no_tvshows) - len(cleaned_movie_names))\n",
    "    else:\n",
    "        cleaned_movie_names = cleaned_movie_names[:len(df_downloads_no_tvshows)]\n",
    "\n",
    "# Add the cleaned names to the dataframe\n",
    "df_downloads_no_tvshows['cleaned_movie_name'] = cleaned_movie_names\n",
    "\n",
    "# Print some results\n",
    "print(df_downloads_no_tvshows[['filename', 'cleaned_movie_name']].head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df_downloads_no_tvshows[['filename', 'cleaned_movie_name']].to_csv('real_debrid_downloads_no_tvshows.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to find the best match\n",
    "def find_best_match(title, dataset):\n",
    "    best_match = None\n",
    "    best_ratio = 0\n",
    "    for _, movie in dataset.iterrows():\n",
    "        ratio = fuzz.token_sort_ratio(title.lower(), movie['title'].lower())\n",
    "        if ratio > best_ratio:\n",
    "            best_ratio = ratio\n",
    "            best_match = movie\n",
    "    return best_match, best_ratio\n",
    "\n",
    "# Find matches for each download\n",
    "matches = []\n",
    "total_downloads = len(df_downloads_no_tvshows)\n",
    "\n",
    "# Create a progress bar\n",
    "with tqdm(total=total_downloads, desc=\"Matching downloads\") as pbar:\n",
    "    for _, download in df_downloads_no_tvshows.iterrows():\n",
    "        best_match, ratio = find_best_match(download['cleaned_movie_name_with_year'], TMDB_dataset)\n",
    "        if best_match is not None:\n",
    "            matches.append({\n",
    "                'download_filename': download['filename'],\n",
    "                'cleaned_filename': download['cleaned_movie_name_with_year'],\n",
    "                'matched_title': best_match['title'],\n",
    "                'match_ratio': ratio,\n",
    "                'release_date': best_match['release_date'],\n",
    "                'vote_average': best_match['vote_average']\n",
    "            })\n",
    "        pbar.update(1)\n",
    "\n",
    "# Create a DataFrame from the matches\n",
    "df_matches = pd.DataFrame(matches)\n",
    "\n",
    "# Sort by match ratio in descending order\n",
    "df_matches = df_matches.sort_values('match_ratio', ascending=False)\n",
    "\n",
    "# Display the top matches\n",
    "print(df_matches.head(20))\n",
    "\n",
    "# Save the matches to a CSV file\n",
    "df_matches.to_csv('real_debrid_tmdb_matches.csv', index=False)\n",
    "print(\"Matches have been saved to 'real_debrid_tmdb_matches.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from fuzzywuzzy import process\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Preprocess TMDB dataset\n",
    "def preprocess_tmdb(df):\n",
    "    df['title_lower'] = df['title'].str.lower()\n",
    "    return df\n",
    "\n",
    "# Find matches for each download\n",
    "def find_matches(downloads, tmdb_data):\n",
    "    matches = []\n",
    "    titles = tmdb_data['title_lower'].tolist()\n",
    "    \n",
    "    for _, download in tqdm(downloads.iterrows(), total=len(downloads), desc=\"Matching downloads\"):\n",
    "        cleaned_name = download['cleaned_movie_name_with_year'].lower()\n",
    "        match = process.extractOne(cleaned_name, titles, score_cutoff=50)\n",
    "        \n",
    "        if match:\n",
    "            matched_title, ratio = match  # Changed this line\n",
    "            idx = titles.index(matched_title)  # Added this line\n",
    "            best_match = tmdb_data.iloc[idx]\n",
    "            matches.append({\n",
    "                'download_filename': download['filename'],\n",
    "                'cleaned_filename': download['cleaned_movie_name_with_year'],\n",
    "                'matched_title': best_match['title'],\n",
    "                'match_ratio': ratio,\n",
    "                'release_date': best_match['release_date'],\n",
    "                'vote_average': best_match['vote_average']\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(matches)\n",
    "\n",
    "# Preprocess TMDB dataset\n",
    "TMDB_dataset_cleaned = preprocess_tmdb(TMDB_dataset)\n",
    "\n",
    "# Find matches\n",
    "df_matches = find_matches(df_downloads_no_tvshows, TMDB_dataset_cleaned)\n",
    "\n",
    "# Sort by match ratio in descending order\n",
    "df_matches = df_matches.sort_values('match_ratio', ascending=False)\n",
    "\n",
    "# Display the top matches\n",
    "print(df_matches.head(20))\n",
    "\n",
    "# Save the matches to a CSV file\n",
    "df_matches.to_csv('real_debrid_tmdb_matches.csv', index=False)\n",
    "print(\"Matches have been saved to 'real_debrid_tmdb_matches.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rapidfuzz import process, fuzz\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess_tmdb(df):\n",
    "    df['title_lower'] = df['title'].str.lower()\n",
    "    return df\n",
    "\n",
    "def vectorized_match(downloads, tmdb_data, batch_size=1000, score_cutoff=0):\n",
    "    matches = []\n",
    "    titles = tmdb_data['title_lower'].tolist()\n",
    "    \n",
    "    print(f\"Number of downloads: {len(downloads)}\")\n",
    "    print(f\"Number of TMDB titles: {len(titles)}\")\n",
    "    print(f\"First few download titles: {downloads['cleaned_movie_name_with_year'].head().tolist()}\")\n",
    "    print(f\"First few TMDB titles: {tmdb_data['title'].head().tolist()}\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(downloads), batch_size), desc=\"Matching downloads\"):\n",
    "        batch = downloads.iloc[i:i+batch_size]\n",
    "        cleaned_names = batch['cleaned_movie_name_with_year'].str.lower().tolist()\n",
    "        \n",
    "        for query, original in zip(cleaned_names, batch.itertuples()):\n",
    "            match_result = process.extractOne(query, titles, scorer=fuzz.WRatio, score_cutoff=score_cutoff)\n",
    "            \n",
    "            print(f\"Query: {query}\")\n",
    "            print(f\"Match result: {match_result}\")\n",
    "            \n",
    "            if match_result:\n",
    "                matched_title, ratio, idx = match_result\n",
    "                best_match = tmdb_data.iloc[idx]\n",
    "                matches.append({\n",
    "                    'download_filename': original.filename,\n",
    "                    'cleaned_filename': original.cleaned_movie_name_with_year,\n",
    "                    'matched_title': best_match['title'],\n",
    "                    'match_ratio': ratio,\n",
    "                    'release_date': best_match['release_date'],\n",
    "                    'vote_average': best_match['vote_average']\n",
    "                })\n",
    "            \n",
    "        #     if len(matches) >= 5:  # Break after 5 matches to avoid flooding output\n",
    "        #         break\n",
    "        # if len(matches) >= 5:\n",
    "        #     break\n",
    "    \n",
    "    print(f\"Number of matches found: {len(matches)}\")\n",
    "    return pd.DataFrame(matches)\n",
    "\n",
    "# Preprocess TMDB dataset\n",
    "TMDB_dataset_cleaned = preprocess_tmdb(TMDB_dataset)\n",
    "\n",
    "# Find matches\n",
    "df_matches = vectorized_match(df_downloads_no_tvshows, TMDB_dataset_cleaned)\n",
    "\n",
    "if df_matches.empty:\n",
    "    print(\"No matches were found. Please check your data and matching criteria.\")\n",
    "else:\n",
    "    df_matches = df_matches.sort_values('match_ratio', ascending=False)\n",
    "    print(df_matches.head(20))\n",
    "    df_matches.to_csv('real_debrid_tmdb_matches.csv', index=False)\n",
    "    print(\"Matches have been saved to 'real_debrid_tmdb_matches.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "#print the total number and percentage of ratio\n",
    "# Calculate statistics for different match ratio thresholds\n",
    "thresholds = [90, 80, 70, 60, 50]\n",
    "total_matches = len(df_matches)\n",
    "\n",
    "print(f\"Total number of matches: {total_matches}\")\n",
    "\n",
    "for threshold in thresholds:\n",
    "    matches_above_threshold = df_matches[df_matches['match_ratio'] >= threshold]\n",
    "    num_matches = len(matches_above_threshold)\n",
    "    percentage = (num_matches / total_matches) * 100\n",
    "    \n",
    "    print(f\"Matches with ratio >= {threshold}: {num_matches} ({percentage:.2f}%)\")\n",
    "\n",
    "# Calculate the average match ratio\n",
    "average_ratio = df_matches['match_ratio'].mean()\n",
    "print(f\"\\nAverage match ratio: {average_ratio:.2f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from MDBList import MDBListAPI\n",
    "import json\n",
    "\n",
    "api = MDBListAPI(\"idtjc1mqrsfyhi7m9l2s1ag23\")\n",
    "\n",
    "# Get all user lists\n",
    "user_lists = api.get_user_lists()\n",
    "print(\"User Lists:\")\n",
    "print(json.dumps(user_lists, indent=2))\n",
    "\n",
    "print(user_lists)\n",
    "\n",
    "# Find the ID of the \"zeroq/new-on-stremio\" list\n",
    "target_list_name = \"zeroq/new-on-stremio\"\n",
    "target_list_id = None\n",
    "\n",
    "for list_info in user_lists.get('data', []):\n",
    "    if list_info.get('name') == target_list_name:\n",
    "        target_list_id = list_info.get('id')\n",
    "        break\n",
    "\n",
    "if target_list_id:\n",
    "    print(f\"\\nFound list ID for '{target_list_name}': {target_list_id}\")\n",
    "\n",
    "    # Get list info\n",
    "    list_info = api.get_list_info(target_list_id)\n",
    "    print(\"\\nList Information:\")\n",
    "    print(json.dumps(list_info, indent=2))\n",
    "\n",
    "    # Get list items\n",
    "    list_items = api.get_list_items(target_list_id)\n",
    "    print(\"\\nList Items:\")\n",
    "    print(json.dumps(list_items, indent=2))\n",
    "else:\n",
    "    print(f\"\\nCould not find list with name '{target_list_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from opensubtitles import OpenSubtitlesClient\n",
    "\n",
    "def main():\n",
    "    client = OpenSubtitlesClient()\n",
    "    \n",
    "    # Search for the movie\n",
    "    movie_query = \"Furiosa.A.Mad.Max.Saga.2024.1080p.WEBRip.1600MB.DD5.1.x264-GalaxyRG.mkv\"\n",
    "    movie_results = client.get_query_results(movie_query)\n",
    "    \n",
    "    if not movie_results:\n",
    "        print(f\"No results found for '{movie_query}'\")\n",
    "        return\n",
    "\n",
    "    # Print the first movie result\n",
    "    first_movie = movie_results[0]\n",
    "    print(f\"Movie found: {first_movie.get('title', 'N/A')}\")\n",
    "    print(f\"IMDB Rating: {first_movie.get('imdb', 'N/A')}\")\n",
    "    print(f\"Movie URL: {first_movie.get('url', 'N/A')}\")\n",
    "    \n",
    "    # Get the movie ID\n",
    "    movie_id = first_movie.get('id')\n",
    "    if not movie_id:\n",
    "        print(\"Could not find movie ID\")\n",
    "        return\n",
    "\n",
    "    # Now search for subtitles using the movie ID\n",
    "    subtitles = client.get_subtitles_by_id(movie_id)\n",
    "    \n",
    "    print(f\"\\nSubtitles for {first_movie.get('title', 'N/A')}:\")\n",
    "    for i, subtitle in enumerate(subtitles, 1):\n",
    "        print(subtitle.get('lang'))\n",
    "        if subtitle.get('lang', 'N/A') == 'Portuguese':\n",
    "            print(f\"\\nSubtitle {i}:\")\n",
    "            print(f\"Language: {subtitle.get('lang', 'N/A')}\")\n",
    "            print(f\"Format: {subtitle.get('sub_type', 'N/A')}\")\n",
    "            print(f\"Download URL: {subtitle.get('download', 'N/A')}\")\n",
    "\n",
    "    print(f\"\\nTotal subtitles found: {len(subtitles)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from opensubtitles import OpenSubtitlesClient\n",
    "\n",
    "def main():\n",
    "    client = OpenSubtitlesClient()\n",
    "    \n",
    "    #print api info\n",
    "    print(client.get_api_info())\n",
    "    \n",
    "    # Search for the movie\n",
    "    movie_query = \"The.Flash.2023.2160p.WEB-DL.DDP5.1.Atmos.DV.HDR.H.265-FLUX.mkv\"\n",
    "    movie_results = client.get_query_results(movie_query)\n",
    "    \n",
    "\n",
    "    # Print the first movie result\n",
    "    first_movie = movie_results[0]\n",
    "    print(f\"Movie found: {first_movie.get('title', 'N/A')}\")\n",
    "    print(f\"IMDB Rating: {first_movie.get('imdb', 'N/A')}\")\n",
    "    print(f\"Movie URL: {first_movie.get('url', 'N/A')}\")\n",
    "    \n",
    "    # Get the movie ID\n",
    "    movie_id = first_movie.get('id')\n",
    "    if not movie_id:\n",
    "        print(\"Could not find movie ID\")\n",
    "        return\n",
    "\n",
    "    # Now search for subtitles using the movie ID\n",
    "    subtitles = client.get_subtitles_by_id(movie_id)\n",
    "    \n",
    "    print(f\"\\nSubtitles for {first_movie.get('title', 'N/A')}:\")\n",
    "    for i, subtitle in enumerate(subtitles[:5], 1):\n",
    "        print(f\"\\nSubtitle {i}:\")\n",
    "        print(f\"Language: {subtitle.get('lang', 'N/A')}\")\n",
    "        print(f\"Format: {subtitle.get('sub_type', 'N/A')}\")\n",
    "        print(f\"Download URL: {subtitle.get('download', 'N/A')}\")\n",
    "\n",
    "    print(f\"\\nTotal subtitles found: {len(subtitles)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Manually specify the path to the directory containing Torrent-Api-py\n",
    "torrent_api_path = '/Users/nuno/programao/torrent_api/Torrent-Api-py'\n",
    "sys.path.append(torrent_api_path)\n",
    "\n",
    "from torrents.pirate_bay import PirateBay\n",
    "\n",
    "async def test_search():\n",
    "    pb = PirateBay()\n",
    "    query = \"avengers\"\n",
    "    page = 1\n",
    "    limit = 5\n",
    "\n",
    "    results = await pb.search(query, page, limit)\n",
    "    print(results)\n",
    "    if results:\n",
    "        print(f\"Search results for '{query}':\")\n",
    "        print(f\"Total results: {results['total']}\")\n",
    "        print(f\"Time taken: {results['time']:.2f} seconds\")\n",
    "        \n",
    "        for idx, torrent in enumerate(results['data'], 1):\n",
    "            print(f\"\\n{idx}. {torrent['name']}\")\n",
    "            print(f\"   Size: {torrent['size']}\")\n",
    "            print(f\"   Seeders: {torrent['seeders']}\")\n",
    "            print(f\"   Leechers: {torrent['leechers']}\")\n",
    "            print(f\"   Category: {torrent['category']}\")\n",
    "            print(f\"   Uploader: {torrent['uploader']}\")\n",
    "            print(f\"   Date: {torrent['date']}\")\n",
    "            if 'quality' in torrent:\n",
    "                print(f\"   Quality: {torrent['quality']}\")\n",
    "            if 'codec' in torrent:\n",
    "                print(f\"   Codec: {torrent['codec']}\")\n",
    "            if 'file_extension' in torrent:\n",
    "                print(f\"   File Extension: {torrent['file_extension']}\")\n",
    "    else:\n",
    "        print(\"No results found or an error occurred.\")\n",
    "\n",
    "# Run the async function\n",
    "asyncio.run(test_search())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://real-debrid.com/streaming-V5KS6PRA3S4QC\"\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7\",\n",
    "    \"accept-encoding\": \"gzip, deflate, br, zstd\",\n",
    "    \"accept-language\": \"en-US,en;q=0.9,pt-BR;q=0.8,pt-US;q=0.7,pt;q=0.6,nl;q=0.5\",\n",
    "    \"cache-control\": \"max-age=0\",\n",
    "    \"cookie\": \"https=1; auth=BUQVTPEI65ZZRCXSR2D36WX7DSUBHCHATTTXRHY; lang=en\",\n",
    "    \"priority\": \"u=0, i\",\n",
    "    \"sec-ch-ua\": '\"Not)A;Brand\";v=\"99\", \"Google Chrome\";v=\"127\", \"Chromium\";v=\"127\"',\n",
    "    \"sec-ch-ua-mobile\": \"?0\",\n",
    "    \"sec-ch-ua-platform\": '\"macOS\"',\n",
    "    \"sec-fetch-dest\": \"document\",\n",
    "    \"sec-fetch-mode\": \"navigate\",\n",
    "    \"sec-fetch-site\": \"none\",\n",
    "    \"sec-fetch-user\": \"?1\",\n",
    "    \"upgrade-insecure-requests\": \"1\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    response = requests.get(url, headers=headers)\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    print(\"Response Headers:\")\n",
    "    for header, value in response.headers.items():\n",
    "        print(f\"{header}: {value}\")\n",
    "    print(\"\\nResponse Content:\")\n",
    "    print(response.text)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Add parsing logic here if needed\n",
    "    else:\n",
    "        print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def download_transcode_and_stream_to_stdout(input_url):\n",
    "    command = [\n",
    "        'ffmpeg',\n",
    "        '-i', input_url,\n",
    "        '-f', 'mpegts',\n",
    "        '-vcodec', 'libx264',\n",
    "        '-acodec', 'aac',\n",
    "        '-b:a', '128k',\n",
    "        'pipe:'\n",
    "    ]\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    return process\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    video_url = 'https://87-4.download.real-debrid.com/d/ZGQ2UUASZII4O/Dragonkeeper.2024.1080p.WEBRip.1400MB.DD5.1.x264-GalaxyRG.mkv'\n",
    "\n",
    "    stream_process = download_transcode_and_stream_to_stdout(video_url)\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            output = stream_process.stdout.read(1024)\n",
    "            if not output:\n",
    "                break\n",
    "    except KeyboardInterrupt:\n",
    "        stream_process.terminate()\n",
    "        stream_process.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import subprocess\n",
    "from urllib.parse import urlparse\n",
    "import sys\n",
    "\n",
    "def download_audio(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    filename = os.path.basename(parsed_url.path)\n",
    "    audio_filename = f\"{os.path.splitext(filename)[0]}.ac3\"\n",
    "\n",
    "    print(f\"Downloading audio from {url}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    ffmpeg_command = [\n",
    "        'ffmpeg',\n",
    "        '-i', url,\n",
    "        '-vn',  # No video\n",
    "        '-acodec', 'copy',  # Copy audio without re-encoding\n",
    "        '-f', 'ac3',\n",
    "        audio_filename\n",
    "    ]\n",
    "\n",
    "    subprocess.run(ffmpeg_command, check=True)\n",
    "    return audio_filename\n",
    "\n",
    "def convert_audio(input_filename):\n",
    "    output_filename = f\"{os.path.splitext(input_filename)[0]}.aac\"\n",
    "\n",
    "    print(\"Converting audio to AAC\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    ffmpeg_command = [\n",
    "        'ffmpeg',\n",
    "        '-i', input_filename,\n",
    "        '-c:a', 'aac',\n",
    "        '-b:a', '128k',\n",
    "        '-f', 'adts',\n",
    "        '-y',\n",
    "        output_filename\n",
    "    ]\n",
    "\n",
    "    subprocess.run(ffmpeg_command, check=True)\n",
    "    return output_filename\n",
    "\n",
    "# Main code\n",
    "video_url = 'https://87-4.download.real-debrid.com/d/ZGQ2UUASZII4O/Dragonkeeper.2024.1080p.WEBRip.1400MB.DD5.1.x264-GalaxyRG.mkv'\n",
    "\n",
    "try:\n",
    "    downloaded_file = download_audio(video_url)\n",
    "    print(f\"Download complete. File: {downloaded_file}\")\n",
    "    \n",
    "    converted_file = convert_audio(downloaded_file)\n",
    "    print(f\"Conversion complete. Output file: {converted_file}\")\n",
    "    \n",
    "    # Optionally, remove the intermediate AC3 file\n",
    "    # os.remove(downloaded_file)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "input_file = \"Dragonkeeper.2024.1080p.WEBRip.1400MB.DD5.1.x264-GalaxyRG.ac3\"\n",
    "output_file = \"Dragonkeeper_2024_audio.aac\"\n",
    "\n",
    "# Construct the FFmpeg command\n",
    "ffmpeg_command = [\n",
    "    \"ffmpeg\",\n",
    "    \"-i\", input_file,\n",
    "    \"-vn\",  # Disable video\n",
    "    \"-acodec\", \"aac\",\n",
    "    \"-b:a\", \"192k\",\n",
    "    \"-f\", \"adts\",\n",
    "    output_file\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Run the FFmpeg command\n",
    "    result = subprocess.run(ffmpeg_command, check=True, capture_output=True, text=True)\n",
    "    \n",
    "    print(f\"Successfully extracted and encoded audio from {input_file} to {output_file}.\")\n",
    "    \n",
    "    # Print FFmpeg output for information\n",
    "    print(\"FFmpeg output:\")\n",
    "    print(result.stdout)\n",
    "\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"An error occurred while running FFmpeg:\")\n",
    "    print(e.stderr)\n",
    "except FileNotFoundError:\n",
    "    print(\"FFmpeg not found. Please ensure FFmpeg is installed and accessible in your system PATH.\")\n",
    "\n",
    "# Check if the output file was created\n",
    "if os.path.exists(output_file):\n",
    "    print(f\"Output file created: {output_file}\")\n",
    "    print(f\"File size: {os.path.getsize(output_file)} bytes\")\n",
    "else:\n",
    "    print(\"Output file was not created. Please check the FFmpeg output for errors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MDBList import MDBListAPI\n",
    "\n",
    "# Initialize the API client\n",
    "api = MDBListAPI(\"idtjc1mqrsfyhi7m9l2s1ag23\")\n",
    "\n",
    "# Get movie info\n",
    "movie_info = api.get_movie_info(\"tt0073195\")\n",
    "print(movie_info)\n",
    "\n",
    "# Search for movies\n",
    "search_results = api.search_movies(\"Jaws\", year=1975)\n",
    "print(search_results)\n",
    "\n",
    "# Get user lists\n",
    "user_lists = api.get_user_lists()\n",
    "print(user_lists)\n",
    "\n",
    "# Get bulk ratings\n",
    "ratings = api.get_bulk_ratings(\"movie\", \"imdb\", [923, 990, 545611])\n",
    "print(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MDBList import MDBListAPI\n",
    "import json\n",
    "\n",
    "# Initialize the API client\n",
    "api = MDBListAPI(\"idtjc1mqrsfyhi7m9l2s1ag23\")\n",
    "\n",
    "# Search for the specified list\n",
    "list_name = \"recommended-new-on-stremio\"\n",
    "search_results = api.search_lists(list_name)\n",
    "\n",
    "print(search_results)\n",
    "\n",
    "user_lists = api.get_user_lists()\n",
    "print(user_lists)\n",
    "\n",
    "# Find the matching list\n",
    "# target_list = None\n",
    "# for list_info in search_results:\n",
    "#     if list_info.get('name') == list_name and list_info.get('username') == list_owner:\n",
    "#         target_list = list_info\n",
    "#         break\n",
    "\n",
    "# if target_list:\n",
    "#     print(\"List found:\")\n",
    "#     print(json.dumps(target_list, indent=2))\n",
    "    \n",
    "#     # Get list items\n",
    "#     list_items = api.get_list_items(target_list['id'])\n",
    "#     print(\"\\nList Items:\")\n",
    "#     print(json.dumps(list_items, indent=2))\n",
    "# else:\n",
    "#     print(f\"List '{list_name}' by '{list_owner}' not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "import io\n",
    "import os\n",
    "import tempfile\n",
    "import webvtt\n",
    "from opensubtitles import OpenSubtitlesClient\n",
    "import time\n",
    "\n",
    "def process_subtitles(movie_title):\n",
    "    client = OpenSubtitlesClient()\n",
    "    processed_subtitles = []\n",
    "\n",
    "    # Get the current directory\n",
    "    current_dir = os.getcwd()\n",
    "\n",
    "    try:\n",
    "        movie_results = client.get_query_results(movie_title)\n",
    "        \n",
    "        if not movie_results:\n",
    "            print(f\"No results found for movie: {movie_title}\")\n",
    "            return []\n",
    "\n",
    "        first_movie = movie_results[0]\n",
    "        movie_id = first_movie.get('id')\n",
    "        \n",
    "        if not movie_id:\n",
    "            print(f\"No movie ID found for: {movie_title}\")\n",
    "            return []\n",
    "\n",
    "        subtitles = client.get_subtitles_by_id(movie_id)\n",
    "\n",
    "        for sub in subtitles:\n",
    "            try:\n",
    "                lang = sub.get('lang')\n",
    "                download_url = sub.get('download')\n",
    "                \n",
    "                if lang and download_url:\n",
    "                    print(f\"Processing subtitle: {lang}\")\n",
    "                    print(f\"Download URL: {download_url}\")\n",
    "                    \n",
    "                    # Download the zip file\n",
    "                    response = requests.get(download_url)\n",
    "                    response.raise_for_status()\n",
    "\n",
    "                    # Extract the subtitle file from the zip\n",
    "                    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "                        subtitle_file = z.namelist()[0]  # Assume the first file is the subtitle\n",
    "                        subtitle_content = z.read(subtitle_file)\n",
    "\n",
    "                    # Save SRT content to a temporary file\n",
    "                    with tempfile.NamedTemporaryFile(mode='w+', suffix='.srt', delete=False, encoding='utf-8') as temp_srt:\n",
    "                        temp_srt.write(subtitle_content.decode('utf-8'))\n",
    "                        temp_srt_path = temp_srt.name\n",
    "\n",
    "                    # Convert SRT to WebVTT\n",
    "                    vtt = webvtt.from_srt(temp_srt_path)\n",
    "\n",
    "                    # Save the WebVTT file in the current directory\n",
    "                    file_name = f\"{movie_title.replace(' ', '_')}_{lang}.vtt\"\n",
    "                    file_path = os.path.join(current_dir, file_name)\n",
    "                    \n",
    "                    vtt.save(file_path)\n",
    "\n",
    "                    # Remove the temporary SRT file\n",
    "                    os.unlink(temp_srt_path)\n",
    "\n",
    "                    # Generate the URL for the subtitle file (using file path)\n",
    "                    subtitle_url = f\"file://{file_path}\"\n",
    "\n",
    "                    processed_subtitles.append({\n",
    "                        'lang': lang,\n",
    "                        'url': subtitle_url\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"Processed subtitle: {processed_subtitles[-1]}\")\n",
    "                    \n",
    "                    time.sleep(5)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing subtitle: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching subtitles: {str(e)}\")\n",
    "\n",
    "    return processed_subtitles\n",
    "\n",
    "# Example usage:\n",
    "movie_title = \"Knives Out\"\n",
    "subtitles = process_subtitles(movie_title)\n",
    "print(f\"Processed subtitles: {subtitles}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
